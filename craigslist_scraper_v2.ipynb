{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code checks job postings on Bay Area Craigslist for several key terms, with an input option to add more.\n",
    "# benjamingibbs.net        github.com/benjaminagibbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pyexcel_ods import get_data\n",
    "from pyexcel_ods import save_data\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here are the keywords and optional additional input\n",
    "keywords = ['lab assistant','graduate','associate engineer']\n",
    "\n",
    "\n",
    "#if input(\"Want to add any search terms? (y/n) \") == 'y':\n",
    "#       keywords.append(input(\"Cool, which ones? \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here's the scrape\n",
    "page = requests.get(\"https://sfbay.craigslist.org/d/jobs/search/jjj\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "sortable_results = soup.find(id=\"sortable-results\")\n",
    "rows = sortable_results.find_all(class_= \"rows\")\n",
    "\n",
    "jobs = rows[0]\n",
    "\n",
    "#uncomment the following line for HTML reference \n",
    "#print(jobs.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = jobs.find_all(\"time\",class_=\"result-date\")\n",
    "titles = jobs.find_all(class_=\"result-title hdrlnk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here's a complete list of all postings for troubleshooting, just uncomment 'skinny'\n",
    "datetime = []\n",
    "post_title = []\n",
    "post_link = []\n",
    "\n",
    "for date in time:\n",
    "   datetime.append(date.get(\"title\"))\n",
    "\n",
    "for title in titles:\n",
    "    post_title.append(title.text)\n",
    "    post_link.append(title.get(\"href\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "skinny = pd.DataFrame({\n",
    "    \"When\":datetime,\n",
    "    \"Title\":post_title,\n",
    "    \"link\":post_link,\n",
    "})\n",
    "#skinny\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [date, link, title]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Here's the parse and check\n",
    "goodjob_name = []\n",
    "goodjob_time = []\n",
    "goodjob_link = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for title in titles:\n",
    "    if any(part in title.text for part in keywords):\n",
    "        goodjob_name.append(title.text)\n",
    "        goodjob_time.append(datetime[i])\n",
    "        #the link is pulled here but not yet included in the final output.\n",
    "        goodjob_link.append(title.get(\"href\"))\n",
    "    i=i+1\n",
    "    \n",
    "good_jobs = pd.DataFrame({\n",
    "            \"title\":goodjob_name,\n",
    "            \"date\":goodjob_time,\n",
    "            \"link\":goodjob_link\n",
    "             })\n",
    "\n",
    "\n",
    "print(good_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we check the existing .ods file, remove duplicates from good_jobs, then append what's left\n",
    "ospath = os.path.join(os.path.expanduser('~'),\"Desktop\",\"scraper-results.csv\")\n",
    "\n",
    "\n",
    "if good_jobs.shape[0] > 0:\n",
    "    n = 0\n",
    "    with open(ospath,'r') as csvfile:\n",
    "        previous = csv.reader(csvfile, delimiter=',')\n",
    "        if not any(good_jobs['title'].tolist() for row in previous):\n",
    "            with open(ospath,'a') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow([goodjob_time[n],goodjob_name[n],goodjob_link[n]])\n",
    "        n = n + 1\n",
    "    csvfile.close()\n",
    "#new_set = previous.append(good_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 0 jobs!\n"
     ]
    }
   ],
   "source": [
    "print(\"found\",len(good_jobs['title'].tolist()),\"jobs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
